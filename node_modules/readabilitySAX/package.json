{
  "name": "readabilitySAX",
  "version": "1.5.3",
  "description": "the readability script ported to a sax parser",
  "author": {
    "name": "Felix Boehm",
    "email": "me@feedic.com"
  },
  "keywords": [
    "html",
    "content extraction",
    "readability",
    "instapaper"
  ],
  "main": "./node/index.js",
  "engine": "node >= 0.4.0",
  "repository": {
    "type": "git",
    "url": "git://github.com/fb55/readabilitysax.git"
  },
  "dependencies": {
    "htmlparser2": "2.x.x",
    "minreq": "~0.1.6",
    "entities": "0.x.x"
  },
  "bin": {
    "readability": "./node/CLI.js"
  },
  "scripts": {
    "test": "node ./tests/test_output.js"
  },
  "readme": "#readabilitySAX\na fast and platform independent readability port\n\n##About\nOne day, I wanted to use [Readability](http://code.google.com/p/arc90labs-readability/), an algorithm to extract relevant pieces of information out of websites, for a node.js project. There are some ports of Readability to node (using jsdom, e.g. [that one](https://github.com/arrix/node-readability)), but they are pretty slow. I don't want to wait for more than a second (literally) until my node instance is ready to continue. So I started this project, porting the code to a SAX parser.\n\nIn my tests, most pages, even large ones, were finished within 15ms (on node, see below for more information). It works with Rhino, so it runs on [YQL](http://developer.yahoo.com/yql \"Yahoo! Query Language\"), which may have interesting uses. And it works within a browser.\n\nThe Readability extraction algorithm was completely ported, but some adjustments were made:\n\n* `<article>` and `<section>` tags are recognized and gain a higher value\n\n* If a heading is part of the pages `<title>`, it is removed (Readability removed any single `<h2>`, and ignored other tags)\n\n* `henry` and `instapaper-body` are classes to show an algorithm like this where the content is. readabilitySAX recognizes them and adds additional points\n\n* Every bit of code that was taken from the original algorithm was optimized, eg. RegExps should now perform faster (they were optimized & use `RegExp#test` instead of `String#match`, which doesn't force the interpreter to build an array)\n\n* Some improvements made by [GGReadability](https://github.com/curthard89/COCOA-Stuff/tree/master/GGReadability) (an Obj-C port of Readability) were adopted\n    * Images get additional scores when their `height` or `width` attributes are high - icon sized images (<= 32px) get skipped\n    * Additional classes & ids are checked\n\n##HowTo\n###Installing readabilitySAX (node)\nThis module is available on `npm` as `readabilitySAX`. Just run \n\n    npm install readabilitySAX\n\n#####CLI\nA command line interface (CLI) may be installed via\n\n    npm install -g readabilitySAX\n\nIt's then available via\n\n    readability <domain> [<format>]\n\nTo get this readme, just run\n\n    readability https://github.com/FB55/readabilitySAX\n\nThe format is optional (it's either `text` or `html`, the default value is `text`).\n\n###Usage\n#####Node\nJust run `require(\"readabilitySAX\")`. You'll get an object containing three methods:\n\n* `Readability(settings)`: The readability constructor. It works as a handler for `htmlparser2`. Read more about it [in the wiki](https://github.com/FB55/readabilitySAX/wiki/The-Readability-constructor)!\n\n* `WritableStream(settings, cb)`: A constructor that unites `htmlparser2` and the `Readability` constructor. It's a writable stream, so simply `.write` all your data to it. Your callback will be called once `.end` was called. Bonus: You can also `.pipe` data into it!\n\n* `createWritableStream(settings, cb)`: Returns a new instance of the `WritableStream`. (It's a simple factory method.)\n\nThere are two methods available that are deprecated and __will be removed__ in a future version:\n\n* `get(link, [settings], callback)`: Gets a webpage and process it.\n\n* `process(data)`: Takes a string, runs readabilitySAX and returns the page.\n\n__Please don't use those two methods anymore__. Streams are the way you should build interfaces in node, and that's what I want encourage people to use.\n\n#####Browsers\n\nI started to implement simplified SAX-\"parsers\" for Rhino/YQL (using E4X) and the browser (using the DOM) to increase the overall performance on those platforms. The DOM version is inside the `/browsers` dir.\n\nA demo of how to use readabilitySAX inside a browser may be found at [jsFiddle](http://jsfiddle.net/pXqYR/embedded/). Some basic example files are inside the `/browsers` directory.\n\n#####YQL\n\nA table using E4X-based events is available as the community table `redabilitySAX`, as well as [here](https://github.com/FB55/yql-tables/tree/master/readabilitySAX).\n\n##Parsers (on node)\nMost SAX parsers (as sax.js) fail when a document is malformed XML, even if it's correct HTML. readabilitySAX should be used with [htmlparser2](https://github.com/FB55/node-htmlparser), my fork of the `htmlparser`-module (used by eg. `jsdom`), which corrects most faults. It's listed as a dependency, so npm should install it with readabilitySAX.\n\n##Performance\n\n#####Speed\nUsing a package of 724 pages from [CleanEval](http://cleaneval.sigwac.org.uk) (their website seems to be down, try to google it), readabilitySAX processed all of them in 5768 ms, that's an average of 7.97 ms per page.\n\nThe benchmark was done using `tests/benchmark.js` on a MacBook (late 2010) and is probably far from perfect.\n\nPerformance is the main goal of this project. The current speed should be good enough to run readabilitySAX on a singe-threaded web server with an average number of requests. That's an accomplishment!\n\n#####Accuracy\nThe main goal of CleanEval is to evaluate the accuracy of an algorithm.\n\n___// TODO___\n\n##Todo\n\n- Add documentation & examples\n- Add support for URLs containing hash-bangs (`#!`)\n- Allow fetching articles with more than one page\n- Don't remove all images inside `<a>` tags",
  "_id": "readabilitySAX@1.5.3",
  "_from": "readabilitySAX"
}
